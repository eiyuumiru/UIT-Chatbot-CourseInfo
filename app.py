# app.py
import streamlit as st
from pathlib import Path
import torch

# ---- import c√°c h√†m / h·∫±ng s·ªë c√≥ s·∫µn trong rag_pipeline.py ----
import rag_pipeline as rag
from rag_pipeline import (
    QA_PROMPT,
    EMBED_MODEL,
    DEFAULT_DEVICE,
    load_index,
)

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.settings import Settings

# --------------------------------- Helper -----------------------------------
@st.cache_resource(show_spinner="ƒêang n·∫°p FAISS index ‚Ä¶")
def init_engine(device: str = "auto", top_k: int = 40):
    """Load FAISS index v√† t·∫°o query_engine."""
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"

    Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL, device=device)
    index = load_index()                          # t·ª´ rag_pipeline.py
    return index.as_query_engine(
        text_qa_template=QA_PROMPT,
        similarity_top_k=top_k,
    )

# --------------------------------- UI ---------------------------------------
st.title("Chatbot h·ªèi ƒë√°p m√¥n h·ªçc UIT")

# ch·ªçn thi·∫øt b·ªã (t√πy b·∫°n gi·ªØ nguy√™n default)
device_opt = st.sidebar.selectbox("Thi·∫øt b·ªã embedding", ["auto", "cpu", "cuda"], index=0)
engine = init_engine(device_opt)

# kh·ªüi t·∫°o session history
if "history" not in st.session_state:
    st.session_state.history = []

# hi·ªÉn th·ªã l·ªãch s·ª≠
for role, msg in st.session_state.history:
    st.chat_message(role).markdown(msg)

# √¥ nh·∫≠p ChatGPT-style
if prompt := st.chat_input("H·ªèi g√¨ v·ªÅ m√¥n h·ªçc? (v√≠ d·ª•: IT003 l√† m√¥n g√¨?)"):
    # l∆∞u & hi·ªÉn th·ªã c√¢u h·ªèi
    st.session_state.history.append(("user", prompt))
    st.chat_message("user").markdown(prompt)

    # tr·∫£ l·ªùi
    with st.chat_message("assistant"):
        with st.spinner("üîç ƒêang t√¨m c√¢u tr·∫£ l·ªùi‚Ä¶"):
            resp = engine.query(prompt)
            answer = str(resp)
            st.markdown(answer)

    # l∆∞u l·ªãch s·ª≠
    st.session_state.history.append(("assistant", answer))
