# app.py
import streamlit as st
from pathlib import Path
import zipfile
import torch
import time

import rag_pipeline as rag
from rag_pipeline import EMBED_MODEL, QA_PROMPT, load_index
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.settings import Settings

# ---------------------------------------------------------------
# ƒê∆∞·ªùng d·∫´n
STORAGE_DIR = Path("storage")
ZIP_PATH     = Path("storage.zip")     # file b·∫°n ƒë√£ commit

# ---------------------------------------------------------------
@st.cache_resource(show_spinner="‚öôÔ∏è Kh·ªüi t·∫°o FAISS index‚Ä¶")
def init_engine(device: str = "auto", top_k: int = 40):
    """ƒê·∫£m b·∫£o c√≥ storage/ (unzip n·∫øu c·∫ßn) r·ªìi t·∫°o query_engine."""
    if not STORAGE_DIR.exists():
        if ZIP_PATH.exists():
            with st.spinner("üì¶ ƒêang gi·∫£i n√©n storage.zip‚Ä¶"):
                with zipfile.ZipFile(ZIP_PATH, "r") as zf:
                    zf.extractall(".")
        else:
            st.error("Kh√¥ng t√¨m th·∫•y storage.zip v√† storage/.")
            st.stop()
        # ƒë·ª£i I/O flush (nh·∫•t l√† tr√™n Streamlit Cloud)
        time.sleep(0.1)

    # ---------- init embedding ----------
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL,
                                                device=device)

    # ---------- load index ----------
    index = load_index()  # t·ª´ rag_pipeline.py
    return index.as_query_engine(text_qa_template=QA_PROMPT,
                                 similarity_top_k=top_k)

# ---------------------------------------------------------------
# UI ƒë∆°n gi·∫£n
st.title("ü§ñ Chatbot m√¥n h·ªçc UIT (RAG + Gemini)")

device_sel = st.sidebar.selectbox("Thi·∫øt b·ªã embedding",
                                  ["auto", "cpu", "cuda"], index=0)

engine = init_engine(device_sel)

if "history" not in st.session_state:
    st.session_state.history = []

for role, msg in st.session_state.history:
    st.chat_message(role).markdown(msg)

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi (vd: IT003 l√† g√¨?)"):
    st.session_state.history.append(("user", prompt))
    st.chat_message("user").markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("üîç ƒêang t√¨m c√¢u tr·∫£ l·ªùi‚Ä¶"):
            resp    = engine.query(prompt)
            answer  = str(resp)
            st.markdown(answer)

    st.session_state.history.append(("assistant", answer))
