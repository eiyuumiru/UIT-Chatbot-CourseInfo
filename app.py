# app.py ‚Äî Streamlit UI cho RAG + Gemini v·ªõi fallback t·ª± ƒë·ªông
import streamlit as st
from pathlib import Path
import os
import torch

# ------ Thi·∫øt l·∫≠p c·∫•u h√¨nh trang (ph·∫£i l√† l·ªánh Streamlit ƒë·∫ßu ti√™n) ------
st.set_page_config(page_title="Chatbot m√¥n h·ªçc UIT", page_icon="ü§ñ")

# Import t·ª´ rag_pipeline c·ªßa b·∫°n
import rag_pipeline as rag
from rag_pipeline import load_index, QA_PROMPT, EMBED_MODEL

# LlamaIndex embedding
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.settings import Settings
# Gemini LLM v√† exception
from llama_index.llms.gemini import Gemini
from google.api_core.exceptions import ResourceExhausted

# ---------- C·∫•u h√¨nh chung ----------
DEVICE = "cpu"                         # CPU-only cho Streamlit Cloud
STORAGE_DIR = Path("storage")         # index ƒë√£ commit

# Danh s√°ch model fallback theo th·ª© t·ª± ∆∞u ti√™n
FALLBACK_MODELS = [
    "models/gemini-2.5-pro",
    "models/gemini-2.5-flash",
    "models/gemini-2.5-flash-lite-preview-06-17",
]

# L·∫•y API key
API_KEY = os.environ.get("GEMINI_API_KEY") or st.secrets.get("GEMINI_API_KEY")

# ---------- Cache index + embedding ----------
@st.cache_resource(show_spinner="‚öôÔ∏è N·∫°p FAISS index & embedding‚Ä¶")
def init_index_and_embedding():
    # C·∫•u h√¨nh embedding t·ª´ rag_pipeline
    Settings.embed_model = HuggingFaceEmbedding(
        model_name=EMBED_MODEL,
        device=DEVICE,
    )
    # Load FAISS index
    return load_index()

# Kh·ªüi t·∫°o index
index = init_index_and_embedding()

# ---------- H√†m query v·ªõi fallback ----------
def query_with_fallback(prompt: str) -> str:
    last_error = None
    for model_name in FALLBACK_MODELS:
        try:
            # C·∫≠p nh·∫≠t LLM
            Settings.llm = Gemini(api_key=API_KEY, model_name=model_name)
            # T·∫°o engine t·ª´ index
            engine = index.as_query_engine(
                text_qa_template=QA_PROMPT,
                similarity_top_k=40,
            )
            # Th·ª±c hi·ªán truy v·∫•n
            response = engine.query(prompt)
            return str(response)
        except ResourceExhausted:
            st.warning(f"‚ö†Ô∏è Quota cho `{model_name.split('/')[-1]}` ƒë√£ h·∫øt, th·ª≠ model kh√°c‚Ä¶")
            last_error = ResourceExhausted(f"Quota h·∫øt cho {model_name}")
        except Exception as e:
            st.error(f"‚ùå L·ªói khi truy v·∫•n model `{model_name}`: {e}")
            raise
    # N·∫øu c·∫£ 3 model ƒë·ªÅu h·∫øt quota
    raise last_error or RuntimeError("T·∫•t c·∫£ model Gemini ƒë·ªÅu h·∫øt quota.")

# ---------- Giao di·ªán chat ----------
st.title("ü§ñ Chatbot m√¥n h·ªçc UIT")

# L·ªãch s·ª≠ chat
if "history" not in st.session_state:
    st.session_state.history = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠
for role, msg in st.session_state.history:
    st.chat_message(role).markdown(msg)

# Nh·∫≠p v√† x·ª≠ l√Ω c√¢u h·ªèi
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi (vd: IT003 h·ªçc g√¨?)"):
    # L∆∞u c√¢u h·ªèi
    st.session_state.history.append(("user", prompt))
    st.chat_message("user").markdown(prompt)

    # Tr·∫£ l·ªùi c√¢u h·ªèi
    with st.chat_message("assistant"):
        with st.spinner("üîç ƒêang t√¨m c√¢u tr·∫£ l·ªùi‚Ä¶"):
            answer = query_with_fallback(prompt)
            st.markdown(answer)

    # L∆∞u c√¢u tr·∫£ l·ªùi
    st.session_state.history.append(("assistant", answer))
