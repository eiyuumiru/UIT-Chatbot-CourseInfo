# app.py ‚Äî Streamlit UI cho RAG + Gemini v·ªõi fallback t·ª± ƒë·ªông
import streamlit as st
from pathlib import Path
import os, torch

# Import t·ª´ rag_pipeline c·ªßa b·∫°n
import rag_pipeline as rag
from rag_pipeline import load_index, QA_PROMPT, EMBED_MODEL

# Thi·∫øt l·∫≠p embedding
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.settings import Settings
# Import Gemini LLM v√† exception
from llama_index.llms.gemini import Gemini
from google.api_core.exceptions import ResourceExhausted

# ---------- C·∫•u h√¨nh chung ----------
DEVICE = "cpu"  # Ch·∫°y tr√™n CPU ƒë·ªÉ ph√π h·ª£p Streamlit Cloud
STORAGE_DIR = Path("storage")  # Th∆∞ m·ª•c index ƒë√£ commit

# Danh s√°ch model fallback theo th·ª© t·ª± ∆∞u ti√™n
FALLBACK_MODELS = [
    "models/gemini-2.5-pro",
    "models/gemini-2.5-flash",
    "models/gemini-2.5-flash-lite-preview-06-17",
]

# L·∫•y API key
API_KEY = os.environ.get("GEMINI_API_KEY") or st.secrets.get("GEMINI_API_KEY")

# ---------- Kh·ªüi t·∫°o m·ªôt l·∫ßn: embedding + FAISS index ----------
@st.cache_resource(show_spinner="‚öôÔ∏è N·∫°p embedding & FAISS index‚Ä¶")
def init_index_and_embedding():
    # C·∫•u h√¨nh embedding model t·ª´ rag_pipeline
    Settings.embed_model = HuggingFaceEmbedding(
        model_name=EMBED_MODEL,
        device=DEVICE,
    )
    # Load index t·ª´ storage/
    index = load_index()
    return index

# Kh·ªüi t·∫°o index & embedding
index = init_index_and_embedding()

# ---------- H√†m query v·ªõi fallback ----------
def query_with_fallback(prompt: str) -> str:
    last_error = None
    for model_name in FALLBACK_MODELS:
        try:
            # C·∫≠p nh·∫≠t LLM trong Settings
            Settings.llm = Gemini(api_key=API_KEY, model_name=model_name)
            # T·∫°o query_engine tr√™n c√πng index
            engine = index.as_query_engine(
                text_qa_template=QA_PROMPT,
                similarity_top_k=40,
            )
            # Th·ª±c hi·ªán truy v·∫•n
            response = engine.query(prompt)
            return str(response)
        except ResourceExhausted:
            # Quota model hi·ªán t·∫°i ƒë√£ h·∫øt, th·ª≠ model k·∫ø
            st.warning(f"‚ö†Ô∏è Quota cho `{model_name.split('/')[-1]}` ƒë√£ h·∫øt, chuy·ªÉn model kh√°c‚Ä¶")
            last_error = ResourceExhausted(f"Quota h·∫øt cho {model_name}")
            continue
        except Exception as e:
            # L·ªói kh√°c th√¨ hi·ªÉn th·ªã v√† d·ª´ng
            st.error(f"‚ùå L·ªói khi truy v·∫•n model `{model_name}`: {e}")
            raise
    # N·∫øu c·∫£ 3 model ƒë·ªÅu h·∫øt quota
    raise last_error or RuntimeError("T·∫•t c·∫£ model Gemini ƒë·ªÅu h·∫øt quota.")

# ---------- Giao di·ªán chat ----------
st.set_page_config(page_title="Chatbot m√¥n h·ªçc UIT", page_icon="ü§ñ")
st.title("ü§ñ Chatbot m√¥n h·ªçc UIT")

# Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
if "history" not in st.session_state:
    st.session_state.history = []

# Hi·ªÉn th·ªã l·ªãch s·ª≠
for role, msg in st.session_state.history:
    st.chat_message(role).markdown(msg)

# Nh·∫≠p c√¢u h·ªèi
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi (vd: IT003 h·ªçc g√¨?)"):
    # L∆∞u v√† hi·ªÉn th·ªã c√¢u h·ªèi
    st.session_state.history.append(("user", prompt))
    st.chat_message("user").markdown(prompt)

    # Tr·∫£ l·ªùi
    with st.chat_message("assistant"):
        with st.spinner("üîç ƒêang t√¨m c√¢u tr·∫£ l·ªùi‚Ä¶"):
            answer = query_with_fallback(prompt)
            st.markdown(answer)

    # L∆∞u c√¢u tr·∫£ l·ªùi v√†o l·ªãch s·ª≠
    st.session_state.history.append(("assistant", answer))
