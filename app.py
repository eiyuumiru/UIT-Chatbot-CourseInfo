# app.py  ‚Äì  Streamlit UI cho RAG + Gemini
import streamlit as st
from pathlib import Path
import torch, time

import rag_pipeline as rag                              # file g·ªëc c·ªßa b·∫°n
from rag_pipeline import EMBED_MODEL, QA_PROMPT, load_index
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.settings import Settings

# ---------- h·∫±ng s·ªë ----------
STORAGE_DIR = Path("storage")         # ƒë√£ commit v√†o repo
DEVICE      = "cpu"                   # Streamlit Cloud ch·ªâ c√≥ CPU

# ---------- kh·ªüi t·∫°o FAISS + embedding (cache) ----------
@st.cache_resource(show_spinner="‚öôÔ∏è ƒêang n·∫°p FAISS index ‚Ä¶")
def init_engine():
    """Load FAISS index v√† t·∫°o query_engine, ch·ªâ ch·∫°y 1 l·∫ßn/session."""
    # ƒê·∫∑t embedding model (intfloat/multilingual-e5-small)
    Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL,
                                                device=DEVICE)

    # N·∫°p index t·ª´ th∆∞ m·ª•c storage/ ƒë√£ c√≥ s·∫µn
    index = load_index()
    return index.as_query_engine(text_qa_template=QA_PROMPT,
                                 similarity_top_k=40)

# ---------- UI ----------
st.set_page_config(page_title="Chatbot m√¥n h·ªçc UIT", page_icon="ü§ñ")
st.title("ü§ñ Chatbot m√¥n h·ªçc UIT (RAG + Gemini)")

# L·ªãch s·ª≠ h·ªôi tho·∫°i
if "history" not in st.session_state:
    st.session_state.history = []     # list[(role, msg)]

# Hi·ªÉn th·ªã l·ªãch s·ª≠
for role, msg in st.session_state.history:
    st.chat_message(role).markdown(msg)

# H·ªôp nh·∫≠p li·ªáu ki·ªÉu ChatGPT
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi (vd: IT003 h·ªçc g√¨?)"):
    # Ghi c√¢u h·ªèi
    st.session_state.history.append(("user", prompt))
    st.chat_message("user").markdown(prompt)

    # L·∫•y engine (cache) & truy v·∫•n
    with st.chat_message("assistant"):
        with st.spinner("üîç ƒêang t√¨m c√¢u tr·∫£ l·ªùi ‚Ä¶"):
            engine  = init_engine()            # cache_resource
            answer  = str(engine.query(prompt))
            st.markdown(answer)

    # Ghi c√¢u tr·∫£ l·ªùi
    st.session_state.history.append(("assistant", answer))
